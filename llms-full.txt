# Keith Szot — Complete Post Archive

This file contains the full text of all posts for AI ingestion.
Generated: 2026-02-10

---

<!-- FULL_CONTENT_START -->

## Esper Deep Cuts: Esper Foundation for Android
**Enterprise-Grade AOSP for Dedicated Device Fleets**

**Date:** 2025-04-23  
**Series:** Esper Deep Cuts  
**Author:** Keith Szot  
**Source:** [Esper Blog](https://www.esper.io/blog/esper-deep-cuts-esper-foundation-for-android)

In today's rapidly evolving landscape of enterprise and advanced consumer electronics, the reliability, security, and flexibility of dedicated Android devices are more critical than ever. Businesses across sectors — restaurants, retail, healthcare, hospitality, and custom product development — depend on these devices to perform without fail.

Esper Foundation for Android is an enterprise-grade operating system built specifically for these scenarios. It ensures seamless operation, robust security, powerful remote management, and streamlined scalability. Whether powering retail POS systems, digital signage, correctional facility tablets, or consumer fitness devices, Foundation is already in action, delivering consistent results.

### Why Foundation Matters

Executives and CTOs in these industries understand the risks associated with operational disruptions. Device downtime, exploitable vulnerabilities, and inefficient management practices lead to dissatisfied customers, regulatory exposure, and increased operational costs. Foundation addresses these challenges through a tightly integrated set of features:

**Enhanced Security:** Foundation includes secure boot, verified over-the-air (OTA) updates with the latest security patches, and comprehensive lockdown capabilities. These features work together to ensure device integrity, protect against unauthorized access, and mitigate risks from vulnerabilities — both known and emerging.

**Advanced Remote Management:** Built to integrate seamlessly with Esper's cloud-based management platform, Foundation enables full remote control of thousands of devices from a central console. This reduces the need for on-site interventions, minimizing downtime — especially vital for enterprises with multiple retail or restaurant locations. It's important to note that Foundation is designed specifically for Esper and must be managed through our platform. It is not intended for standalone use or for use with other MDM providers.

**Flexible Customization:** Foundation gives businesses the ability to tailor device behavior and workflows to their exact needs. Whether configuring a restaurant ordering system, retail kiosk, healthcare endpoint, or consumer product, Foundation makes it possible to deliver consistent, engaging, and brand-specific experiences.

**Efficient Staging and Deployment Maintenance:** With Esper Seamless Provisioning built directly into the OS, Foundation simplifies the staging process while reducing both time and cost. Once deployed, devices are self-healing. Foundation uses Foundry — a scalable OS validation and deployment pipeline tailored for enterprise — to securely deliver OS updates. This process de-risks critical deployments by preventing bricking scenarios and maintaining uninterrupted service. This is especially important for customers in healthcare and hospitality, where uptime and compliance are non-negotiable.

**Simplified Compliance:** Foundation helps reduce the administrative burden of maintaining regulatory compliance with standards like HIPAA and PCI-DSS. By design, Foundation simplifies device management workflows, reducing the risk of non-compliance and making audit processes more straightforward.

**True Android, with More:** Foundation is not a fork of Android. It is full AOSP, enhanced with features specific to dedicated device deployments. These enhancements exist alongside the standard Android API set and provide additional capabilities that extend Android's usefulness in mission-critical environments.

### The Foundation of Foundation

Foundation is built on full AOSP — it is not a fork, and it maintains complete compatibility with the Android ecosystem. However, Foundation must be built specifically for the silicon and hardware platform it will run on. Each supported device family corresponds to a "branch" of our internal source code tree, and these branches may contain "leaves" — customizations specific to customer use cases.

Each of these branches and leaves is actively maintained. Every quarter, we deliver updated base builds with security patches and create standalone installable images and OTA packages for in-field updates.

Because Foundation serves vertical markets where Android version requirements are tightly coupled to supported application stacks, we support multiple Android versions simultaneously. Today, Google provides security patch coverage for Android 12 through Android 15, so our team provides backporting services for customers requiring ongoing security updates for older Android versions. This means that even if a version falls outside of Google's standard patch window, our engineers can apply applicable patches to keep your devices secure.

To visualize Foundation's architecture, consider Android as a layer cake:

- The Linux kernel forms the base
- The vendor layer (typically maintained by the silicon vendor or OEM) sits above that
- The Android Framework is the top layer — this is where Esper adds value

While the kernel is largely community-maintained, and the vendor layer is developed by the hardware provider, Esper fully maintains the Android Framework on Foundation builds. In select cases — particularly for Foundation x86 — we also manage the kernel and vendor layers, which allows us to fully support and deliver OTA updates for the entire stack. In these scenarios, we coordinate closely with silicon vendors and OEMs to ensure compatibility and stability.

While we typically deliver security updates quarterly (aligned with customer expectations), our infrastructure is capable of monthly updates, trailing Google's Android Security Bulletin by 30–60 days. For high-severity vulnerabilities (Sev0), we can deliver off-cycle patches as needed, though this hasn't yet been required.

Foundation's OTA infrastructure is deeply integrated with Esper's DevOps pipeline. Through our Foundry system, we ensure each OTA is validated, packaged, and presented cleanly via the Esper Console. This enables customers to easily test updates in a lab environment and then use Esper Pipelines to orchestrate a phased rollout across fleets with fine-grained control.

Foundation supports both push and pull OTA configurations, allowing for different update strategies depending on your operational needs. Pull-based updates can be particularly useful for consumer electronics deployments. We also support A/B partitioned updates — if the hardware allows for it — to ensure continuity of service during OS updates.

As your solution evolves, Foundation is built to evolve with it. We support upgrades between Android versions using the same OTA framework. These migrations are complex and require joint planning with the customer and OEM, but we've successfully helped customers move to newer Android "dessert" releases in production environments.

### Foundation: Extending AOSP for Dedicated Use Cases

#### Seamless Provisioning

Provisioning AOSP devices through standard methods can be tedious and operationally inefficient. Unlike GMS-based Android Enterprise, which offers various provisioning options, AOSP provisioning traditionally requires enabling Developer Options and connecting each device via USB, Ethernet, or Wi-Fi to a developer machine using ADB.

Esper Seamless Provisioning eliminates this overhead by allowing devices to auto-provision at first boot. Operators register devices using serial numbers or IMEIs — via manual input or CSV upload — and pre-assign Blueprints and tags. Upon first boot, each device connects to the Esper Cloud, is matched to its tenant and Blueprint, and is provisioned with policies, settings, and apps.

This approach transforms staging: devices can be drop-shipped and set up on-site by non-technical staff. For example, a convenience store customer used this model to install kiosks in minutes, with store managers handling installation.

For OEMs, Foundation can be flashed at the factory with no dependency on existing Blueprints. Devices can be boxed, shipped through the supply chain, and ready for configuration once received. Customers often ask for a manifest that correlates serial numbers to individual units, allowing them to pre-assign Blueprints and ensure a smooth rollout.

Foundation also builds resiliency into the provisioning process. If a device is factory reset in the field, it will re-provision using the assigned Blueprint — preventing unauthorized use and maintaining IT control.

Connectivity is often a question here. For Ethernet devices (common in Foundation deployments), network access is automatic. For Wi-Fi-only devices, credentials can be embedded into the Foundation build to allow seamless first-time provisioning.

#### Foundation SDK and Esper Device SDK

Foundation includes a set of APIs through the Foundation SDK, giving enterprise developers access to configuration controls not typically exposed by AOSP:

- Application-driven control of the navigation bar, including the ability to hide or show it dynamically
- App data backup and restore, enabling controlled downgrades without losing user data (if compatible)
- Remote customization of the boot animation, including the ability to change it post-deployment

In addition, the Esper Device SDK, which is embedded in Foundation, extends control to standard Android app-level code, offering features such as:

- Adjusting screen brightness, orientation, and timeout
- Triggering reboots, locking/unlocking the device, and toggling the custom Esper Settings app
- Fetching device-specific settings and telemetry
- Toggling Bluetooth and Wi-Fi states, managing mobile data and Wi-Fi hotspots
- Granular USB control, including whitelisting for a specific app or managing permissions
- App lifecycle control (clearing data, state management, permission grants, managed configs)
- Managing Access Point Names (APNs) on the device

These SDK capabilities create deep opportunities for operational tuning and automation, critical in large-scale fleet environments.

#### Secure Remote ADB and WebADB (Preview)

Foundation supports secure remote ADB, providing powerful troubleshooting capabilities with the safety of remote toggling. By default, remote ADB can be disabled to block local USB-based access and can be re-enabled through Esper Console when diagnostics are required. This makes it possible to investigate complex device issues without physical access.

In limited preview, we also offer WebADB, which brings remote debugging capabilities directly into the Esper Console through the browser — no terminal or SDK tools required. It's an especially convenient option for distributed support teams.

#### Additional Advanced Capabilities

Foundation includes a growing set of capabilities designed specifically for real-world fleet operations:

- **Mesh Networking Auto-Discovery:** Foundation devices on the same LAN subnet can discover and securely communicate with each other, enabling peer-based configurations for restaurants, QSRs, or retail locations.
- **Titanium Browser:** A secure, Chromium-based browser maintained by Esper and optimized for kiosks and other dedicated use cases, addressing the lack of viable browser options in the AOSP ecosystem.
- **WEST Web App Support:** Esper's Web App generation tool (WEST) offers similar functionality to Google's Managed Play web app tool — except it works for AOSP — enabling customers to create and manage web apps without requiring GMS.
- **802.1X Ethernet Authentication:** Enterprise-grade Ethernet security not natively supported in AOSP, available in Foundation for secure public deployments (stadiums, kiosks, etc.).
- **Persistent Log Buffers:** Standard Android logcat buffers are small and cleared on reboot, making post-mortem debugging difficult. Foundation extends this buffer significantly (targeting seven days) and includes an option for persistence across reboots. You also get access to `wpa_cli` and `dmesg` logs for deep Wi-Fi and kernel-level debugging.
- **Esper Port Manager:** Enables Linux-level peripheral access from Android apps, bridging gaps for legacy hardware — crucial for vertical deployments with specialized or non-standard hardware interfaces.

### Foundation x86

The Foundation x86 branch was developed in close collaboration with Intel and is designed to run on 64-bit x86 processors. Unlike our ARM builds, which rely on vendor-provided kernel and HAL components, Esper maintains the entire Android operating system stack on Foundation x86 — including the kernel, vendor layer, and Android Framework. This full-stack ownership gives us unparalleled control and flexibility.

Foundation x86 is widely adopted in retail and restaurant environments, especially for POS systems, ordering kiosks, and digital media players powered by Intel NUCs. It installs directly on bare metal — no virtualization required — and is factory-load ready. This makes it easy to integrate with OEM validation, burn-in testing, and production workflows.

While we're often asked about minimum hardware requirements, x86 systems vary too widely to publish a strict list. That said, we've successfully run Foundation x86 on 15+ year-old hardware with as little as 2GB of RAM. Of course, this limits performance and is only suitable for low-intensity workloads. We previously maintained a 32-bit version, but most customers have since migrated to 64-bit, and we've deprecated the 32-bit branch.

Foundation x86 is highly adaptable to specialized hardware. Devices with unique configurations — such as dual displays or peripherals connected via RS-485, serial, or parallel ports — can be supported through custom engineering. We offer packaged services to develop the necessary drivers and integration layers. While it's rare, some scenarios may require cooperation with the peripheral manufacturer. We evaluate these dependencies as part of our pre-sales due diligence.

To streamline development, we provide a watermarked build of Foundation x86 to qualified prospects for early testing. We recommend performing a bare metal install on your target system to validate compatibility and performance. BIOS/UEFI drift between x86 machines can introduce inconsistencies, so hands-on testing is the best way to identify and resolve issues early.

One important consideration: apps running on Foundation x86 must include the `x86_64` Application Binary Interface (ABI). This is a simple configuration change in Android Studio, but it's essential for compatibility. While Intel's Houdini binary translator allows ARM apps to run on x86 hardware, Foundation does not include Houdini by default. It can be added with additional engineering effort (via Intel) if absolutely necessary. However, we recommend building for `x86_64` natively to avoid translation overhead.

Although Foundation is based on AOSP, it can be used as a base for GMS builds — particularly under Google's EDLA (Enterprise Device Licensing Agreement) program. Since GMS is layered on top of AOSP, Foundation can be made EDLA-compliant through testing and validation. Note that Esper cannot certify GMS compliance directly — that's the responsibility of the EDLA licensee — but we can help run CTS and GMS test suites to prepare a Foundation build for GMS certification.

For OEMs or enterprises with highly specific hardware or performance requirements, we can create a custom "leaf" off the Foundation x86 branch. This allows for targeted customization and maintenance, including the generation of distinct OTA packages. However, these leaves require dedicated engineering resources to build, validate, and support over time.

### Foundation x86 Flip

Many enterprise customers — particularly in retail and restaurant sectors — are looking to migrate their existing Windows-based endpoints to Android. However, hardware replacement is expensive and disruptive. With Foundation x86 Flip, you don't need to rip and replace. Instead, you can remotely convert your Windows systems to run Foundation x86.

Here's how it works:

- A small bootstrap file is deployed to each Windows machine using your existing remote management tool.
- The bootstrap initiates the conversion, either shrinking the Windows partition to preserve it or performing a clean install.
- The device reboots into the Foundation installer.
- Once installed, Seamless Provisioning completes the setup using the assigned Blueprint.

The only requirement is associating the device's serial number with the appropriate Blueprint in advance. Once that's done, the process is fully touchless. Customers have reported a 95%+ success rate with this approach — most issues stem from BIOS drift or edge-case system configurations.

For situations where remote bootstrap isn't feasible, we've developed Firebolt, a USB-based utility that packages the entire conversion workflow. You configure Firebolt with your desired install parameters (e.g., preserve Windows or wipe clean), plug it into the target system, and let it handle the install and provisioning automatically.

### Foundation Emulator for Windows x86

In some deployment environments, everything runs on Android — except for one critical device, such as a manager's PC that must remain on Windows due to productivity tool dependencies. To bridge this gap, we developed the Foundation Emulator for Windows x86.

This emulator runs a fully managed, locked-down instance of Foundation on a Windows machine. From the perspective of Esper, it behaves just like a physical Android device:

- It's assigned a Blueprint
- It receives policy, settings, and apps
- It can be updated via OTA

We even solved the challenge of LAN integration by enabling the emulator to use the Windows machine's NIC as a full network participant.

Think of it as an AOSP tablet that lives on your Windows desktop — managed, locked down, and secured by Esper.

### Foundation GSI for Arm

With the introduction of Project Treble in Android 8, Google introduced a modular architecture that separates the Android OS framework from the device-specific vendor implementation. This change was made to accelerate Android updates and simplify the maintenance lifecycle for device makers.

One key outcome of Project Treble is the ability to use a Generic System Image (GSI) — a complete, hardware-agnostic Android OS build that can be flashed onto any Treble-compliant device. With a GSI, you can replace the OEM-provided OS while leaving the vendor partition untouched.

Esper offers Foundation GSI for Arm as a way to deploy Foundation on Treble-compliant hardware without requiring deep customization or full re-engineering. This is especially useful for scenarios where customers want to leverage off-the-shelf Android devices (e.g., prosumer-grade tablets) but need a locked-down, enterprise-ready OS.

A common example: correctional facilities using Android tablets. Many consumer-grade tablets offer the right performance and price point, but the stock OS is too permissive and hard to control. By flashing Foundation GSI onto those devices, you get a secure, manageable experience — complete with Esper OTA support, remote management, and Blueprint-based configuration.

To install a Foundation GSI, you'll need the appropriate flashing tools provided by the silicon vendor. Some OEMs may restrict flashing, but most allow it when done properly. Caution is warranted here — flashing an incompatible GSI on a non-compliant device can result in a brick. We've seen this happen with devices that claim Treble compliance (`adb shell getprop ro.treble.enabled = true`) but fail in practice.

To help mitigate risk, Google offers a Dynamic System Update (DSU) mechanism. DSU lets you boot a GSI on supported Android 10+ devices without overwriting the original OS. It's a non-destructive test mode that enables quick evaluation. If the Foundation GSI works via DSU, you're in good shape to proceed with a full install.

DSU requires sufficient free storage space, and once testing is complete, you can remove the GSI by disabling it in Developer Options or via ADB. The device will then revert to its original OS with no data loss.

If you're unsure whether your hardware is compatible, Google provides Reference GSIs for Android 9–15. These can be used as a quick test: if the reference GSI boots successfully, Foundation GSI likely will too. If it doesn't, that's our problem to solve — not yours.

Foundation GSI brings the power of Esper to a wider range of Arm devices without requiring a bespoke build. It's a great fit for customers looking to scale with cost-effective hardware while still enforcing policy, security, and update control.

### Foundation for Amlogic – Khadas VIM3 Line

Amlogic chipsets are commonly found in Android TV sticks and other low-cost digital media devices. However, these devices typically come with consumer-oriented firmware built for 10-foot UI experiences — not the best fit for enterprise use cases like digital signage or menu boards.

That said, the price point is compelling. Through collaboration with early customers, we identified the Khadas VIM3 line as a suitable platform for Foundation. VIM3 boards are powered by Amlogic and offer sufficient performance, availability, and build quality for dedicated deployments.

Esper has worked directly with Khadas to bring up Foundation on the VIM3. While these devices require some light preparation — such as optional enclosures or heatsinks — they remain far more cost-effective than NUCs or other premium media players.

The key advantage? You're not relying on questionable firmware that could introduce security risks into your network. Instead, you get Foundation — complete with OTA update infrastructure, remote management, Blueprint provisioning, and tight control.

While newer Arm-based digital signage hardware is emerging with better out-of-box support, VIM3 remains a proven option. Several of our customers have deployed it in the field for digital menu boards and signage at scale.

### Foundation Bespoke for Arm

Some OEMs are building highly specialized devices for enterprise or consumer markets — devices with no standard screen, unique input methods, and purpose-built hardware configurations. In these cases, the stock Android OS provided by the silicon vendor often falls short. It lacks the customization, polish, and control required to create a seamless product experience and manage it at scale.

That's where Foundation Bespoke for Arm comes in.

If the OEM has access to the Android vendor layer, Esper can deliver a fully customized build of Foundation. This includes not just OS-level enhancements, but also full branding and UI control. The device can be tailored end to end — from boot animation and first-boot experience to home screen layout, system dialogs, and update flows.

On top of this, all Esper device management capabilities are baked in, giving the OEM or enterprise full control over policies, software updates, provisioning, and troubleshooting.

Because Foundation Bespoke is built from the ground up, it's the most engineering-intensive branch in the Foundation family. It requires a dedicated bring-up effort and ongoing maintenance investment, and has the least leverage from existing Foundation codebases. However, for customers deploying at scale with a custom hardware design, the upfront cost is often outweighed by the long-term value of control, manageability, and brand fidelity.

We've brought Foundation Bespoke to life on a wide range of platforms, including MediaTek, Qualcomm, Synaptics, and even Rockchip. The custom nature of this engagement means we won't have a prebuilt Foundation image ready to test on your hardware. That said, we can provide a watermarked build from another Foundation branch to help evaluate Esper's management experience and overall platform approach.

### How to Try Foundation

If any of these Foundation branches sparked your interest — whether you're deploying at scale, upgrading from Windows, or developing a custom product — let's talk.

Contact us and share your target hardware, use case, and any specific requirements. We'll work with you to determine the right branch, discuss engineering needs, and (where applicable) get a watermarked build in your hands for evaluation.

Some of these discussions may require an NDA, and we're happy to accommodate that.

**Foundation on.**

---

## Android OTA at the Edge – CTO Edition
**A CTO's Guide to Updates as Infrastructure**

**Date:** 2026-02-09  
**Series:** Esper Field Notes  
**Author:** Keith Szot  
**Source:** [Substack](https://keithszot.substack.com/p/android-ota-at-the-edge-cto-edition)

When Android devices are used as infrastructure rather than consumer phones, a failed update stops being an annoyance and becomes a business incident. Revenue stops, operations stall, and the trucks roll. Nothing about Android announces this shift. You discover it later, usually during an update that was supposed to be routine.

This is why OTA ends up on a CTO's desk. Not because it is unusually complex, but because it hardens decisions early and charges interest on them over time.

### OTA Is a Boundary, Not a Feature

Android OTA sits at the intersection of several optimization models that were never designed to align.

Google optimizes for global security velocity. Patches ship monthly. Critical fixes move fast. At global scale, delay is unacceptable.

Enterprise IT optimizes for stability. Updates are staged, deferred, and gated by policy. Risk is managed through control and timing.

Dedicated fleets optimize for predictability. Devices are expected to remain unchanged unless there is a compelling operational reason. Reboots are events, not inconveniences. Validation precedes rollout by weeks or months.

All three models coexist inside modern Android. Problems emerge when authority between them is assumed rather than designed.

The executive question here is not how updates work. It is who is allowed to change a device, under what conditions, and with what visibility.

### The Factory Image Is the Base Layer

The most consequential OTA decisions are made before a device ever ships.

Partition layout. Update strategy. Flash sizing. Rollback protection. Kernel configuration. Verification policy. These are not implementation details. They determine which futures remain possible.

Once devices are deployed, these choices cannot be revisited. A system that cannot roll back safely will never gain that ability. A device that cannot accommodate OS growth will eventually hit a hard limit.

Many organizations inherit these decisions from ODMs, BSP vendors, or reference designs. That inheritance often optimizes for cost and time to market, not lifecycle survivability.

A CTO does not need to argue about code here. They need to understand which tradeoffs were made and which ones are now locked in. If those answers are unclear, the organization is already operating on borrowed assumptions.

### A/B, Virtual A/B, and Failure Margin

Early Android devices updated in place. If something went wrong, recovery was manual.

A/B updates were introduced to improve survivability. The device maintains two system slots. Updates are written to the inactive slot, verified, and activated on reboot. If the new slot fails, the system can roll back automatically.

This changes the shape of failure. It does not eliminate it.

Reboots are still required. Applications do not continue running across slot switches. Any appearance of continuity is created by device configuration and management policy, not by live OS hot swapping.

Virtual A/B extends the model using snapshot and copy-on-write techniques to reduce flash overhead. It trades simplicity for efficiency and relies more heavily on kernel and filesystem behavior under stress.

Single-slot devices still exist, particularly in cost-sensitive designs. They are simpler and cheaper. They are also far less forgiving. When updates fail, recovery options narrow quickly.

These are not abstract distinctions. They determine whether a bad update becomes an automated rollback or a field incident.

### Flash Math Always Wins

Flash capacity and partitioning quietly determine how long a device can evolve.

On A/B systems, effective system capacity is split. As Android grows, vendor components expand, and security hardening increases, updates that once fit eventually do not.

On single-slot systems, update scratch space is finite. Overshoot it and the update fails, or worse, partially applies.

Dynamic partitions help by allowing system components to share space over time, but only if they were designed in from the beginning. They do not create storage. They delay rigidity.

This is one of the most common slow failures in Android fleets. Nothing breaks suddenly. One day, the math stops working. That moment was always coming. The only mitigation is treating flash headroom as a first-class constraint in the factory image, not an afterthought validated by whoever had the spreadsheet last.

### OTA Is State Evolution

OTA systems do not operate on blank devices. They operate on accumulated history.

Manufacturing revisions, silent hardware changes, prior updates, repairs, and partial recoveries all contribute to divergence. Devices that appear identical in dashboards are often not identical internally.

Incremental updates assume an exact starting state, enforced through build fingerprints and version lineage. When those assumptions drift, targeting becomes fragile.

This is how fleets become difficult to reason about. Not because engineers are careless, but because state compounds quietly. Over time, OTA stops being delivery and starts looking like controlled mutation.

### Authority Lives in the Keys

Android enforces update authority cryptographically. OTAs signed with the platform key are treated as the OS speaking to itself. They can modify any privileged component and permanently alter the trust boundary below the application layer.

This authority is necessary. It is also absolute.

Governance failures often hide here. OTA capability can exist in places executives do not expect: inherited BSP tooling, reference clients left in images, endpoints no one remembers owning. The presence of cryptography does not imply the presence of control.

At this layer, ambiguity is the risk. If it is unclear who holds the keys, where they are stored, and what systems are allowed to use them, ownership has already leaked.

### Applications Survive. Behavior Doesn't Always.

Most OS updates preserve applications and data. That is not the same as preserving outcomes.

Major Android version changes deprecate APIs, tighten permissions, and remove access to system identifiers. Applications built against older assumptions may continue to run while silently failing at the behavior the business depends on.

From the outside, the update looks successful. From the inside, the solution is broken.

This is where OTA crosses out of platform engineering and into business continuity. Updating the OS without understanding application dependencies produces systems that appear healthy until they aren't.

### GMS and AOSP Encode Different Trust Models

The difference between GMS and AOSP devices is not primarily about features. It is about authority.

On GMS devices, Google retains the ability to update certain OS components independently of the OEM. This improves global security responsiveness and reduces systemic risk at scale. It also introduces an external actor into the update lifecycle whose priorities may not align with dedicated fleet uptime expectations.

On AOSP devices, Google is absent. Nothing changes unless the OEM or operator explicitly causes it to change. Predictability increases. Responsibility becomes total.

Neither model is inherently superior. They fail in different ways. Treating them as equivalent guarantees surprise.

### Fragmentation Is the Cost of Freedom

AOSP provides update mechanisms, not governance. Outside the GMS ecosystem, OTA implementations vary widely. This fragmentation is most visible on Android x86 platforms, where there is little ecosystem gravity and no common reference model.

But it appears on ARM as well. Two devices with the same model number and the same Android version can behave differently under OTA because one shipped with a BSP that included dynamic partitions and another shipped without.

OTA quality depends entirely on organizational discipline. When that discipline erodes, systems become dependent on individuals, undocumented knowledge, and fragile assumptions. Over time, those assumptions surface as outages that no single team can explain.

### MDM, Update Policy, and the Limits of Control

Android does expose system update controls to enterprise mobility management systems. Through System Update Policy, an MDM can define maintenance windows, defer updates, or block user initiated OS changes.

These controls are real. They are also not absolute.

They apply only to update paths designed to respect them. They do not override signing authority or privileged update mechanisms.

This gap became visible during what became known as the reboot heard around the world. Google Play System Updates triggered reboots outside approved maintenance windows. Google's response was explicit. This behavior was working as designed.

The lesson was not that MDM is broken. It was that MDM does not own the device.

Vertical market OEMs responded by building deeper control surfaces. Samsung Knox, Zebra LifeGuard, and others expose proprietary APIs that allow deferral, version pinning, controlled rollout, and in some cases downgrade paths. These controls are often surfaced through OEMConfig, creating the appearance that MDM owns the behavior when it is actually acting as a conduit.

From a CTO perspective, policy does not trump platform authority. If an update is signed and delivered through a privileged path, it executes unless the OEM deliberately constrained it.

On pure AOSP devices, the model simplifies but responsibility increases. Nothing changes unless you designed it that way.

OTA outcomes are determined by the narrowest of three layers: what the OS makes possible, what the OEM makes controllable, and what the MDM can govern.

### Where This Leaves a CTO

Android OTA is not complex because engineers enjoy complexity. It is complex because it spans security, hardware, applications, and real world operations simultaneously.

The question is not whether devices can be updated. The question is whether the organization understands which decisions are irreversible, where authority actually lives, and how failure propagates once assumptions break.

Esper Foundation for Android exists to remove this class of problem from the CTO's critical path. It turns implicit decisions into explicit ones and provides a governed starting point for the OS itself.

You still make tradeoffs. They are just visible and deliberate.

Once devices are in the field, you do not get to redesign the control surface. You discover how much of one you actually have.

### Appendix: Questions a CTO Should Be Able to Ask

These are pressure questions, not trivia. You do not need perfect answers. You need answers that cohere.

#### Factory Image and Survivability
Is the device single slot or A/B? If A/B, is it full or virtual? How much flash headroom exists today? Are dynamic partitions enabled? What rollback protections are enforced?

#### Authority and Governance
Who holds the platform signing keys? Where are they stored? What systems are authorized to use them? Are there inherited OTA clients from BSPs or vendors?

#### Update Operations
How are updates staged and rolled out? What telemetry is monitored? What conditions halt deployment? What happens if an update partially succeeds?

#### Application Impact
Which applications depend on privileged APIs or system identifiers? How are OS updates coordinated with application owners? How is silent breakage detected?

#### Lifecycle and Security
How are monthly Android security bulletins handled? Which fixes are backported versus forcing version jumps? What happens when a branch can no longer be patched safely?

#### Recovery and Downgrades
Is downgrade supported? Under what conditions? Does recovery require physical access? What is the operational cost at scale?

#### MDM and Update Authority
Which OTA mechanisms respect System Update Policy? Which bypass it? How are conflicts resolved? What OEM specific controls exist, and are they enforced at the OS level?

If a supplier cannot answer these questions clearly, they are not hiding anything malicious. They are revealing where responsibility ends and assumptions begin.

That boundary is exactly what a CTO needs to see.

---

## Android OTA at the Edge – OEM Edition
**What Changes When Android Devices Become Infrastructure**

**Date:** 2026-02-09  
**Series:** Esper Field Notes  
**Author:** Keith Szot

When you ship an Android device for use as a kiosk, a payment terminal, or a warehouse scanner, that device isn't a phone. It's infrastructure. And once it's infrastructure, OTA stops being a feature you ship. It becomes a competitive advantage you either have or you don't.

Not all OEMs have figured this out.

### The Split That's Breaking Your Margins

You're caught in the middle of a culture collision. Google optimizes Android for velocity. Patch aggressively, ship fast, assume connectivity and user acceptance. Enterprise customers demand the opposite: controlled rollouts, tight maintenance windows, the ability to defer updates. Vertical market customers live even farther out: no changes unless there's a compelling reason, and even then, only under conditions they dictate.

Consumer Android, enterprise Android, and vertical-market Android want fundamentally different things from OTA. You're trying to serve all three with the same platform.

Most OTA failures don't happen because updates are technically hard. They happen because you're attempting to satisfy these three audiences simultaneously using a stack that was designed for one of them.

Here's what that looks like in practice: Google ships a security bulletin every month. In some situations you may have 30 days to integrate patches or your customers get upset. But those patches don't just land in AOSP cleanly. They land everywhere: platform layer, vendor layer, kernel tree. Your kernel team is on a different cadence. Your vendor blob is closed-source. Your BSP might be months behind on the kernel baseline. Meanwhile, your vertical market customers are asking why you can't just "pause" the Google update pipeline for their fleet.

You can't, and your IT Ops customers feel it.

On the GMS side, you don't even have that much control. Google operates its own update plane. Project Mainline patches, Google Play System Updates that can trigger reboots on your customer's devices without your approval, without a maintenance window, without their consent. Your customer's kiosk goes dark mid-transaction. They call you. Your answer is "as designed."

This isn't wrong. Google is optimizing for global security. But your customer isn't completely interested in global security. They're interested in their uptime and KPIs.

This is the core OEM problem: you've outsourced OTA governance to Google, to your BSP vendor, to whoever controls your kernel tree. You're assembling pieces and hoping they align. Most of the time they are slightly askew.

### The Factory Image Is Where Your Destiny Actually Lives

OTA doesn't start when you upload an update. It starts before the device boots for the first time.

The partition layout. A/B versus A-only. Dynamic partitions. Verified Boot configuration. Rollback indices. Kernel strategy. These are not procurement decisions. These are OTA strategy decisions, and once devices ship, they're irreversible.

You made them years ago. You might not even remember making them. But your field teams live with them every single day.

An A-only device with fixed partitions and undersized flash capacity is now shipping to your largest customer, and a security patch just won't fit. You can't apply the update, defer it cleanly on GMS devices, or roll it back. You're out of options.

This is what happens when you let the ODM or BSP vendor design the factory image and you don't push back. Most OEMs defer this decision. It's easier to let Qualcomm's BSP or Rockchip's reference design dictate partition strategy. Modern Qualcomm platforms handle this well. Rockchip platforms are more variable. You may ship some devices with dynamic partitions and some without, even under the same model number and Android version.

Your supply chain team thinks that's fine. Your engineering team knows it's a problem. Your customer discovers it at 2 AM when an OTA fails in the field right before holiday lockdown.

This is also where Esper Foundation for Android enters the picture, not as an add-on but as a foundational decision. Foundation means you own the build and the partition strategy. You understand the hardware constraints from day one and design OTA expectations around them.

That's not a checkbox. It's a competitive moat.

OEMs using Foundation can ship devices with deliberate, validated OTA strategies baked in. OEMs relying on off-the-shelf BSPs are hoping the partition math works out.

### When Your OTA Manager Isn't Even Yours

Here's something that should concern you: you might not actually know who controls OTA on your devices.

AOSP provides the update engine, payload formats, and Verified Boot integration. It does not provide an OTA system. There is no orchestration, telemetry, policy gating, or governance layer by default.

So someone fills the gap. Sometimes you build it. Sometimes your ODM does. Sometimes your silicon vendor embeds one in the BSP. Sometimes all three.

We've seen OEMs discover that a Rockchip BSP includes an OTA client already embedded in the system image. Nobody at the OEM explicitly chose it. Nobody owns it operationally. It simply exists, connected to infrastructure the OEM does not control.

Whoever controls OTA signing controls the platform. They can update anything below the application sandbox. A malicious OTA signed with your platform key is indistinguishable from a legitimate one.

If you don't control the signing infrastructure, you don't control your OS lifecycle.

Most OEMs don't discover this fragmentation until something breaks and they have to untangle who actually owns the failure.

### GOTA and Operational Governance in Android Fleets

Google's GOTA API is attractive. It offloads infrastructure, provides orchestration tooling, and allows OEMs to focus on hardware and integration.

For consumer devices and many general enterprise deployments, that tradeoff makes sense.

Vertical markets are different.

Fleet operators deploy devices across thousands of locations. They operate in narrow service windows and validate extensively before rollout. Surprise reboots are not just inconvenient. They break operations.

GOTA is optimized for Google's priorities: rapid global security response and consistency at scale. Vertical fleets prioritize predictability, staged rollout, and tightly controlled change windows. Those priorities are not wrong. They are not always aligned.

MDM policies do not govern Google Play System Updates. GOTA does not remove Google's authority to deliver Mainline updates that may require reboots.

For OEMs supporting mixed fleets of GMS and AOSP devices, the challenge compounds. You are managing two governance models under one roof.

AOSP offers a different tradeoff. No Google update plane. No Mainline. No parallel authority. Every update path is explicit. Every reboot is deliberate.

But AOSP shifts responsibility entirely to the OEM. Every patch, every security fix, every lifecycle decision is yours.

Many OEMs avoid AOSP because it appears to require more upfront discipline. Often that's true. But GMS does not eliminate complexity. It redistributes it.

OEMs targeting vertical markets increasingly find that AOSP paired with a disciplined OS foundation like Esper Foundation aligns better with long-lived fleet operations by consolidating update authority.

### The Patch Clock Never Stops

Every 30 days, a new Android Security Bulletin lands. Within roughly 48 hours, patches hit AOSP. Your integration timeline depends on certification status, vendor readiness, and how many branches you support. The clock starts regardless.

On GMS, that pressure multiplies. Platform patches, vendor patches, kernel patches, and Google's own update plane all move on different cadences.

Some fixes backport cleanly. Many do not. Some require architectural changes that only exist in newer releases. Others are blocked by closed-source vendor components.

Every month you choose: backport aggressively or force a major version jump.

Customers notice which choice you make.

Esper Foundation changes the math by allowing disciplined backporting across platform, vendor, and kernel layers as a coherent whole.

### AOSP Ecosystems Are Fragmented by Default

There is no standard OTA model for AOSP.

AOSP provides machinery, not a system. Two devices running the same AOSP version can have radically different OTA behavior depending on ODM configuration and infrastructure.

On platforms without GMS, especially Intel x86-based Android systems, fragmentation is even more pronounced. There is no gravitational pull toward consistency.

This fragmentation is painful, but it is also an opportunity.

The OEMs winning in vertical markets are not following templates. They are building intentional OTA systems aligned to how their customers actually operate.

### Your Competitors Are Already Moving

Zebra LifeGuard is not just an update tool. It is a declaration of ownership over the OS lifecycle.

Samsung Knox E-FOTA, Honeywell, Datalogic, Panasonic all follow the same pattern. They control the build, the keys, the partitions, the update paths.

Their customers pay for that control because the alternative costs more.

### The Decision You're Making

You can compete on price by shipping close to stock and deferring OTA complexity.

Or you can compete on reliability and control by owning the build, the OTA system, and the lifecycle.

Esper Foundation exists for OEMs choosing the second path.

### What's Coming

The OEMs that win vertical markets will not have the longest feature lists. They will have the most control.

Control over updates, recovery, and lifecycle.

That control starts with the factory image and continues through deliberate infrastructure choices.

Once devices are in the field, you don't get to redesign the control surface. You discover how much of one you actually have.

---

## AI at the Edge: The Real State of Play - OEMs
**Part 2: OEMs, Lifecycles, and the CTO's Actual Problem**

**Date:** 2026-02-03  
**Series:** AI at the Edge: The Real State of Play (Part 2)  
**Author:** Keith Szot  
**Source:** [Substack](https://keithszot.substack.com/p/ai-at-the-edge-the-real-state-of-b31)

If you are a CTO with an edge AI mandate and an installed base of devices, your first problem is not AI models, frameworks, or vendors. It's where inference runs and how much of your existing fleet you are willing, or able, to change.

Most enterprises don't operate on AI data-center timelines. Retail, hospitality, healthcare, logistics, and industrial environments plan on five-, seven-, or even ten-year device lifecycles. AI, by contrast, is moving on consumer-like timelines. Silicon generations turn over annually. Toolchains quickly evolve. Capabilities leap forward and then age out almost immediately by vertical market standards.

The mismatch of long-lived fleets colliding with fast-moving AI is the defining tension shaping OEM behavior today.

This post is not a catalog of vendors. It is a field guide to understanding how today's OEM realities, silicon availability, and inference architecture shape the decisions CTOs are actually being forced to make.

We look at OEMs through a deliberately narrow lens: how their current platforms, lifecycles, and architectural choices shape what is possible for AI at the edge. It is not an assessment of overall product quality, market leadership, or customer success for the endpoints these OEMs offer.

None of the tradeoffs described here reflect lack of OEM ambition, they reflect the responsibility of serving vertical market customers.

### The Edge AI Questions CTOs Are Asking

When organizations say "we need to do edge AI," what they usually mean is one of four things:

Can we run inference on the devices we already have?

Do we need new hardware to do this responsibly?

Should we bypass endpoints and run inference locally on-prem instead?

Or should we centralize inference in the cloud and accept the tradeoffs?

OEMs matter because they largely determine the cost, timing, and risk of every answer.

### Decide Where Inference Runs

There are three anchor locations for inference at scale.

On-device inference offers the lowest latency and strongest privacy guarantees, but it creates the hardest lifecycle problem when capabilities are rapidly changing. This path only works when hardware access, OS support, and OEM capabilities align and when teams accept tight coupling between models, runtimes, and OS updates.

On-prem edge inference introduces a local compute node in a store, restaurant, warehouse, or facility. This preserves existing endpoints while modernizing compute. For many enterprises, this has become the most pragmatic way to introduce edge AI without triggering a mass hardware refresh.

Cloud inference is the fastest way to start and the easiest to scale. Latency, bandwidth costs, data gravity, and outage tolerance become the tradeoffs.

A practical deployment strategy is to go hybrid. Time-critical or privacy-sensitive inference stays local. Heavier, bursty, or non-urgent workloads go to the cloud. The real architectural question is not where inference can run, but when does it move?

OEM constraints strongly influence how cleanly that routing can be implemented.

### Enterprise Device OEMs: The Installed Base Reality

Zebra (including Elo), Honeywell, Datalogic, Getac, MicroTouch, Toshiba Global Commerce Solutions

Enterprise device OEMs optimize for predictability, certification, and survivability. Their customers expect devices to run reliably for years, often in harsh environments, with minimal operational drama.

Zebra and Honeywell increasingly ship Android devices built on Qualcomm platforms with accessible NPUs and GPUs. These platforms do not seek pure benchmark numbers. They are tuned for sustained performance under thermal and power constraints. Vision workloads, OCR, barcode interpretation, and task-specific inference dominate.

Datalogic occupies a distinct position shaped by its deep machine-vision heritage. Many of its platforms rely on disciplined CPU and GPU pipelines rather than headline AI accelerators. For customers already running deterministic vision systems, AI arrives as augmentation for improving accuracy and flexibility without discarding proven workflows.

MicroTouch, along with vendors like Touch Dynamic and Custom America, represents a class of systems with long "dwell" times and more flexible platform strategies than many traditional rugged OEMs. MicroTouch frequently adopts Android and Linux configurations built on MediaTek Genio-class SoCs, where integrated AI accelerators are readily available and exposed. Touch Dynamic and Custom America tend to follow similar architectural patterns, pairing displays and fixed systems with configurable compute rather than tightly integrated, closed platforms.

Getac, by contrast, spans Qualcomm-based Android devices and Intel-based Windows systems, allowing AI capability to enter through multiple paths, either via Qualcomm NPUs on Android or CPU- and GPU-based inference on Windows.

OEMs like Aava Mobile sit in an interesting middle ground. Android-first, but enterprise-disciplined, they tend to adopt Qualcomm platforms that expose AI acceleration without forcing consumer-driven refresh cycles. For teams that want more control than mainstream consumer Android allows without drifting into bespoke hardware this class of OEM offers a pragmatic path forward.

Toshiba's retail platforms, carrying forward the lineage of IBM's store systems, emphasize long-term availability and operational predictability. AI is introduced carefully here, often via CPU inference or adjacent compute rather than specialized accelerators.

Across this category, the pattern is consistent: AI capability is arriving, but it is shaped by the realities of long-lived fleets.

### The Low-Risk On-Ramps to Edge AI

#### Kiosks and Drive-Through
GRUBBR, Frank Mayer, Olea, Posiflex, Acrelec

Kiosks and drive-through systems are not the most glamorous edge AI endpoints, but they are among the most forgiving. They are fixed in place, power-rich, thermally stable, and modular by design.

This modularity is the quiet reason so much edge AI experimentation shows up here first. Compute can be introduced, upgraded, or removed without destabilizing the enclosure, the UI, or the surrounding workflow. Dropping in an x86 box, a NUC, or even a GPU-equipped sidecar is operationally tractable.

AI adoption in kiosks is often driven less by the use case than by the ease of implementation. The same dynamic applies to drive-through systems, where cameras, microphones, and local compute can be combined with relatively low risk.

#### Kitchen Display Systems and Back-of-House Operations
KitchenArmor, Epson

The highest economic impact of AI in hospitality doesn't have to appear in the customer experience. It can be behind the scenes.

Kitchen throughput, prep timing, ingredient usage, labor allocation, and anomaly detection are where AI delivers durable value. Kitchen display systems sit at the center of these workflows.

Notably, most KDS platforms today do not ship with native, general-purpose AI acceleration by design. These systems are built to be reliable execution surfaces, not experimental compute nodes.

In practice, the intelligence does not live in the display, it integrates with it. Most successful deployments rely on remote inference (CPU-based, edge-node-based, or cloud-driven) to serve KDS platforms reliably without compromising uptime or predictability. PerfectCo is one example of an ISV delivering AI-like capabilities using existing hardware designs.

#### Digital Signage and Ambient Intelligence
Instore Screens and Advanced Signage Platforms

Digital signage has quietly become one of the most AI-active edge categories. Audience analytics, dwell measurement, and content adaptation are inherently local problems. Latency and privacy matter.

Vendors like Instore Screens (available through Lenovo) are shipping compact signage controllers, such as FlexBox, with integrated NPUs, designed to run inference locally while fitting cleanly into enterprise procurement and lifecycle models. These systems illustrate how AI can be introduced without turning displays into complex endpoints.

In many ways, signage shows the end state other edge categories are still moving toward: intelligence placed close to the interaction surface, compute treated as modular infrastructure, and endpoints kept simple enough to survive long lifecycles.

### Platformized Retail Systems: Stability Over Silicon

Toast, Square, Clover

These platforms are deliberately conservative at the endpoint not because they lack ambition, but because their business models reward stability. They operate large, tightly managed estates where recurring revenue comes from software, services, and transactions, not hardware refresh.

Thus their velocity comes from product decisions, not device churn. Intelligence is pushed upward into tightly controlled cloud services, where it can be updated continuously and monetized predictably. Endpoints remain stable so operations remain predictable.

This is not a limitation, it's a design choice. For many operators, AI arriving as a service rather than as a hardware capability is the correct trade.

### Enterprise Vertical Infrastructure: Trust and Time

Global Payments (Xenial), NCR Voyix, PAR, Diebold Nixdorf, Oracle MICROS

These vendors operate at a different layer of the stack. They run the operational and transactional backbone of retail, hospitality, and financial self-service environments, often at national or global scale. In these systems, customers punish instability far more than they reward novelty.

The hardware platforms in this category are intentionally conservative. Xenial's GC26, for example, is built on long-life Intel x86 architectures optimized for predictability, availability, and certification. While these systems do not chase the latest AI accelerators, they provide stable, well-understood compute environments that can reliably host CPU-based inference, orchestration logic, and integration points for adjacent edge or cloud intelligence.

AI adoption here is deliberate by necessity. These vendors tend to introduce intelligence at the system and workflow level first spanning demand forecasting, optimization, fraud detection, labor and inventory analytics often with inference running nearby rather than embedded directly in the endpoint.

When AI arrives in this layer, it tends to be durable rather than flashy. CTOs should expect progress on timelines that respect trust, compliance, and operational risk and platforms that are designed to support AI as a long-lived capability, not a transient feature.

### Edge Compute as a Pressure Valve

#### HP and Lenovo: Two Paths to Enterprise Edge AI

HP and Lenovo both make visible a pattern many enterprises are converging on but they arrive there from different starting points.

Lenovo operates across a remarkably broad hardware spectrum. On one side of the business, it ships large volumes of PC-like endpoints into retail and hospitality environments: Windows POS systems, NUC-like form factors, and a wide range of tablets, including devices used directly in vertical workflows. On the other side, Lenovo has invested heavily in edge infrastructure through its ThinkEdge portfolio, including compact systems capable of hosting discrete GPUs.

This breadth gives enterprises optionality. Modest AI capability can exist at the endpoint through integrated NPUs in Intel Core Ultra or Qualcomm-based designs while heavier inference workloads are pulled inward to edge nodes that can be upgraded independently. Lenovo's approach assumes heterogeneity and embraces architectural flexibility.

HP's path is more focused. HP does not meaningfully participate in vertical tablets, but it has deep roots in fixed-location retail hardware and is actively energizing its edge business around AI-driven solutions. Recent organizational changes reflect a push to treat AI not as a device feature, but as part of a broader retail and edge offering.

In practice, this means HP often concentrates intelligence adjacent to endpoints rather than distributing it widely. Compact edge systems and accelerator-equipped platforms become the natural home for generative and multimodal workloads, while POS and other endpoints remain stable and predictable.

Both approaches converge on the same operational truth: distributing GPUs across thousands of endpoints rarely scales. Architecturally, x86 expandability is powerful. Operationally, enterprises prefer concentrating complexity where compute can be cooled, serviced, upgraded, and governed without destabilizing the fleet.

### Payment Terminals: Compliance Defines the Boundary

Verifone, PAX, Ingenico, Castles, BBPOS

Payment terminals operate under some of the strictest regulatory regimes in the enterprise landscape. While many now run Android-based operating systems, any AI execution on the terminal itself remains tightly constrained by certification, security, and compliance requirements.

Dedicated AI accelerators, if present at all, are rarely exposed. CPU-based processing dominates, typically sandboxed and vendor-controlled. Vision, biometrics, and fraud-related capabilities tend to be fixed-function rather than general-purpose.

As a result, intelligence around payments increasingly lives outside the terminal in adjacent services responsible for identity, risk assessment, policy enforcement, and orchestration. The terminal's role narrows toward secure execution, while decision-making logic moves upward in the stack.

CTOs should assume payment terminals will primarily consume AI services rather than host them, a shift that is already reshaping how value and responsibility are distributed across the payments ecosystem.

### Emerging Android-First OEMs: Speed With Responsibility

iMin, Bluebird, SUNMI, Newland, Chainway, Urovo

This class of OEMs adopts MediaTek and Qualcomm silicon aggressively and tends to expose NPUs earlier than traditional enterprise hardware vendors. From a pure capability standpoint, many are ahead of the installed base when it comes to on-device AI potential.

Within this group, there are meaningful differences. Vendors like iMin stand out for taking a more integrated, product-driven approach pairing MediaTek platforms with thoughtful industrial design and a tighter hardware–software story. The result feels less like a collection of SKUs and more like a deliberate ecosystem, which makes emerging AI capabilities easier to reason about and experiment with.

Others in the category optimize for speed and scale, moving quickly to new silicon and form factors, often with impressive hardware at aggressive price points. The tradeoff across the class is lifecycle ownership. Faster refresh cycles and shorter guarantees shift more responsibility onto the customer.

These platforms excel at pilots, innovation-forward deployments, and localized rollouts. Scaling them into long-lived, globally managed fleets is possible but it requires discipline and strong fleet controls.

### CPU-Only Inference: How to Know If You Already Have Enough

A surprising amount of useful edge AI does not require NPUs or GPUs.

Modern CPUs can handle quantized (lower precision) vision models, OCR, anomaly detection, recommendation systems, and task-specific language models. For many enterprises, the real question is not whether CPUs can do AI, but what kinds of AI they tolerate.

Hardware refreshed within the last two to three years is often more capable than teams expect. In practice, compute limits are frequently encountered later than memory limits. Adequate RAM, storage throughput, and IO bandwidth often matter as much as raw CPU performance.

For CTOs looking to evaluate this quickly, the simplest test is not a benchmark, it is a workload probe. Take one representative use case. Run it locally. Observe latency, concurrency, and failure behavior under realistic load. If inference remains predictable and does not starve the rest of the system, you likely have more headroom than you assumed.

Teams evaluating CPU-only inference should focus less on theoretical peak performance and more on workload characteristics: model size, quantization level, latency tolerance, and how many requests must be served simultaneously. Many vertical use cases fit comfortably within these constraints when expectations are grounded in operational reality.

CPU inference is not a fallback. It is often how AI ships on enterprise timelines quietly, incrementally, and without forcing premature hardware decisions.

### Refresh, Pattern, and the Shape of a Sustainable Strategy

Hardware refresh is not the enemy of edge AI. It is often the enabler.

Newer platforms bring better thermals, improved acceleration, and longer runway. OEMs have invested heavily in making AI-capable hardware available across categories, and those advances matter. The opportunity lies in refreshing intentionally. Not all at once (unless you've had a capital budget windfall or Finance wants to take advantage of accelerated depreciation opportunities), but in ways that compound value over time.

Across successful deployments, a consistent pattern emerges. Endpoints remain modest. Inference is treated as a service. Local edge compute absorbs change.

Phased refresh, targeted upgrades, and aligning new hardware with inference placement allow enterprises to introduce AI without unnecessary disruption. Edge compute acts as a bridge between generations, enabling new capabilities to arrive before every endpoint is replaced.

When inference can be discovered, routed, and upgraded independently of endpoints, AI becomes something enterprises can actually operate and not a one-time upgrade, but an evolving capability that survives hardware cycles.

### What This Means Going Forward

OEMs are not blocking AI at the edge. They are shaping it under real constraints.

Consumer silicon will always move first. Vertical silicon will follow. Long-life enterprise SKUs will lag both. That sequence is structural, not accidental.

CTOs succeed by designing architectures that survive that reality and by refreshing hardware in ways that compound advantage rather than reset it. Compounding, in this context, means each generation of hardware expands what is possible without forcing wholesale replatforming. It means inference placement, fleet management, and operational controls remain stable even as silicon capabilities improve underneath them.

When refresh cycles add capacity without invalidating prior decisions, AI becomes an evolving capability instead of a recurring disruption.

Throughout this post, OEMs were discussed only in terms of how their platforms enable or constrain AI at the edge. In many other dimensions, these same vendors differentiate in ways that matter deeply to customers. This lens was intentionally narrow, because AI capability is now intersecting with hardware lifecycles in ways that cut across otherwise strong product strategies.

The next layer down, the silicon vendors, is where these dynamics originate. That's where we turn next!

### A Note on Timing

One last thought, and it's less about technology than timing.

Most edge AI conversations happen too late. They start after hardware has already been selected, budgets allocated, and refresh cycles locked in. At that point, teams are no longer designing systems, they're negotiating constraints.

The most productive conversations happen earlier when fleets are still in motion, when inference placement is still flexible, and when the difference between a three-year decision and a seven-year one is negotiable.

At Esper, we spend a lot of time in that early window, often before there's an RFP or a hardware refresh is officially on the calendar. The work is less about selling software and more about helping teams reason clearly about what they already have, what they're likely to need next, and where AI actually belongs in their architecture.

If this post surfaced questions about your current fleet, your next refresh, or how AI fits into decisions you haven't made yet, I'm always happy to talk. These conversations are most useful early before decisions harden and options narrow.

You can reach me directly, or connect with us at Esper. Either way, the earlier the conversation starts, the more room there is to get it right.

---
<!-- FULL_CONTENT_END -->